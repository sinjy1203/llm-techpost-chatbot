import os
import asyncio
import logging
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from langchain_core.messages import HumanMessage
from langfuse import get_client
from langfuse.langchain import CallbackHandler
from dotenv import load_dotenv

from setup_prompt import setup_system_prompt
from workflow import ReactAgent
from workflow.tools import *
from workflow.state import State

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(override=True)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

LLM_MODEL = os.getenv("LLM_MODEL")
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE"))
DENSE_MODEL = os.getenv("DENSE_MODEL")
SPARSE_MODEL = os.getenv("SPARSE_MODEL")
QDRANT_URL = os.getenv("QDRANT_URL")
MAX_EXECUTE_TOOL_COUNT = int(os.getenv("MAX_EXECUTE_TOOL_COUNT"))


# ì „ì—­ ë³€ìˆ˜ë¡œ agent ì €ì¥
agent_graph = None
langfuse_client = None
langfuse_handler = None


class ChatRequest(BaseModel):
    query: str = Field(
        ..., 
        description="ìœ ì €ì˜ ì§ˆë¬¸", 
        min_length=1,
        example="AI ëª¨ë¸ì˜ ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ì„ ë¹„êµí•´ì£¼ì„¸ìš”"
    )


class ChatResponse(BaseModel):
    answer: str = Field(..., description="ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ")


@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI lifespan ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬"""
    global agent_graph, langfuse_client, langfuse_handler
    
    logger.info("ğŸ¤– Agent ì´ˆê¸°í™” ì¤‘...")
    
    langfuse_client = get_client()
    setup_system_prompt(langfuse_client)
    langfuse_handler = CallbackHandler()

    agent_graph = ReactAgent(
        model_kwargs={
            "model": LLM_MODEL,
            "temperature": LLM_TEMPERATURE
        },
        tools=[
            GeminiSearchTool(
                qdrant_url=QDRANT_URL,
                dense_model_name=DENSE_MODEL,
                sparse_model_name=SPARSE_MODEL
            ),
            OpenaiSearchTool(
                qdrant_url=QDRANT_URL,
                dense_model_name=DENSE_MODEL,
                sparse_model_name=SPARSE_MODEL
            ),
            AnthropicSearchTool(
                qdrant_url=QDRANT_URL,
                dense_model_name=DENSE_MODEL,
                sparse_model_name=SPARSE_MODEL
            ),
            WebSearchTool()   
        ]
    )
    
    logger.info("âœ… Agent ì´ˆê¸°í™” ì™„ë£Œ!")
    
    yield
    
    # ì¢…ë£Œ ì‹œ ì •ë¦¬ ì‘ì—…
    logger.info("ğŸ”„ Agent ì¢…ë£Œ ì¤‘...")
    agent_graph = None


# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="ê¸°ìˆ ë¸”ë¡œê·¸ RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ API",
    description="Gemini, OpenAI, Anthropic ê¸°ìˆ ë¸”ë¡œê·¸ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸",
    version="1.0.0",
    lifespan=lifespan
)


@app.get("/")
async def root():
    """API ìƒíƒœ í™•ì¸"""
    return {"message": "ê¸°ìˆ ë¸”ë¡œê·¸ RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ APIê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤."}


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
    global agent_graph, langfuse_handler
    
    if agent_graph is None:
        raise HTTPException(status_code=500, detail="Agentê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        initial_state = State(
            messages=[HumanMessage(content=request.query)],
            execute_tool_count=0
        )
        
        # ì„¤ì •ê°’
        config = {
            "configurable": {
                "max_execute_tool_count": MAX_EXECUTE_TOOL_COUNT,
                "langfuse_client": langfuse_client
            },
            "callbacks": [langfuse_handler],
            "metadata": {
                "langfuse_tags": ["api"]
            }
        }
        
        # Agent ì‹¤í–‰
        result = await agent_graph.ainvoke(initial_state, config)
        
        # ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ ì¶”ì¶œ
        final_answer = result["messages"][-1].content
        
        if not final_answer:
            raise HTTPException(status_code=500, detail="ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return ChatResponse(answer=final_answer)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "healthy",
        "agent_initialized": agent_graph is not None,
        "langfuse_client": langfuse_client is not None,
        "langfuse_handler": langfuse_handler is not None
    }


@app.post("/prompt/reload")
async def reload_prompt():
    """í”„ë¡¬í”„íŠ¸ ì¬ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸"""
    global langfuse_client

    langfuse_client = get_client()
    
    if langfuse_client is None:
        raise HTTPException(status_code=500, detail="Langfuse í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # í”„ë¡¬í”„íŠ¸ ìºì‹œ ê°•ì œ ìƒˆë¡œê³ ì¹¨
    try:
        langfuse_client.get_prompt(
            "react-agent-system-prompt", 
            type="chat", 
            cache_ttl_seconds=0  # ìºì‹œ ê°•ì œ ìƒˆë¡œê³ ì¹¨
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í”„ë¡¬í”„íŠ¸ ìºì‹œ ìƒˆë¡œê³ ì¹¨ ì‹¤íŒ¨: {str(e)}")
    
    return {"message": "í”„ë¡¬í”„íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì¬ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤."}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000) 