import os
import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from langchain_core.messages import HumanMessage
from dotenv import load_dotenv

from workflow import ReactAgent
from workflow.tools import *
from workflow.state import State

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(override=True)

LLM_MODEL = os.getenv("LLM_MODEL")
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE"))
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL")
QDRANT_URL = os.getenv("QDRANT_URL")
MAX_EXECUTE_TOOL_COUNT = int(os.getenv("MAX_EXECUTE_TOOL_COUNT"))


# ì „ì—­ ë³€ìˆ˜ë¡œ agent ì €ì¥
agent_graph = None


class ChatRequest(BaseModel):
    query: str = Field(
        ..., 
        description="ìœ ì €ì˜ ì§ˆë¬¸", 
        min_length=1,
        example="AI ëª¨ë¸ì˜ ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ì„ ë¹„êµí•´ì£¼ì„¸ìš”"
    )


class ChatResponse(BaseModel):
    answer: str = Field(..., description="ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ")


@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI lifespan ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬"""
    global agent_graph
    
    print("ğŸ¤– Agent ì´ˆê¸°í™” ì¤‘...")

    agent_graph = ReactAgent(
        model_kwargs={
            "model": LLM_MODEL,
            "temperature": LLM_TEMPERATURE
        },
        tools=[
            GeminiSearchTool(
                qdrant_url=QDRANT_URL,
                embedding_model=EMBEDDING_MODEL
            ),
            OpenaiSearchTool(
                qdrant_url=QDRANT_URL,
                embedding_model=EMBEDDING_MODEL
            ),
            AnthropicSearchTool(
                qdrant_url=QDRANT_URL,
                embedding_model=EMBEDDING_MODEL
            )
        ]
    )
    
    print("âœ… Agent ì´ˆê¸°í™” ì™„ë£Œ!")
    
    yield
    
    # ì¢…ë£Œ ì‹œ ì •ë¦¬ ì‘ì—…
    print("ğŸ”„ Agent ì¢…ë£Œ ì¤‘...")
    agent_graph = None


# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="ê¸°ìˆ ë¸”ë¡œê·¸ RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ API",
    description="Gemini, OpenAI, Anthropic ê¸°ìˆ ë¸”ë¡œê·¸ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸",
    version="1.0.0",
    lifespan=lifespan
)


@app.get("/")
async def root():
    """API ìƒíƒœ í™•ì¸"""
    return {"message": "ê¸°ìˆ ë¸”ë¡œê·¸ RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ APIê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤."}


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
    global agent_graph
    
    if agent_graph is None:
        raise HTTPException(status_code=500, detail="Agentê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        initial_state = State(
            messages=[HumanMessage(content=request.query)],
            execute_tool_count=0
        )
        
        # ì„¤ì •ê°’
        config = {
            "configurable": {
                "max_execute_tool_count": MAX_EXECUTE_TOOL_COUNT
            }
        }
        
        # Agent ì‹¤í–‰
        result = await agent_graph.ainvoke(initial_state, config)
        
        # ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ ì¶”ì¶œ
        final_answer = result["messages"][-1].content
        
        if not final_answer:
            raise HTTPException(status_code=500, detail="ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return ChatResponse(answer=final_answer)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "healthy",
        "agent_initialized": agent_graph is not None
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000) 